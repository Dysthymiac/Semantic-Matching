{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Detection visualization with GMM-colored patches and re-ID evaluation.\"\"\"\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from src.config.config import MainConfig\n",
    "from src.data.preprocessed_dataset import PreprocessedDataset\n",
    "from src.data.coco_loader import COCOLoader\n",
    "from src.pca.incremental_pca import IncrementalPCAProcessor\n",
    "from src.codebook.gmm_trainer import load_gmm_model\n",
    "from src.visualization import (\n",
    "    draw_bbox, draw_patches, patch_coords_in_crop, get_crop_bounds,\n",
    "    compute_patch_responsibilities, responsibilities_to_colors\n",
    ")\n",
    "from src.evaluation import match_detections_to_gt, get_identity_mapping, get_image_uuid_from_detection_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config and models\n",
    "config = MainConfig.from_yaml(Path(\"config_zebra_test.yaml\"))\n",
    "# config = MainConfig.from_yaml(Path(\"/fs/ess/PAS2136/ggr_data/GZGC/config_zebra_test.yaml\"))\n",
    "dataset = PreprocessedDataset(config.output_root)\n",
    "pca = IncrementalPCAProcessor(config.pca, config.output_root)\n",
    "gmm, gmm_metadata = load_gmm_model(config.gmm_model_path)\n",
    "\n",
    "# Load COCO annotations and match detections to GT\n",
    "coco_loader = COCOLoader(config.coco_json_path, config.dataset_root)\n",
    "matched = match_detections_to_gt(dataset, coco_loader, iou_threshold=0.5, category_ids=[1])  # 1 = zebra_grevys\n",
    "identity_map = get_identity_mapping(matched)\n",
    "\n",
    "print(f\"Loaded {dataset.get_total_detection_count()} detections\")\n",
    "print(f\"Matched to GT: {len(matched)} ({len(matched)/dataset.get_total_detection_count()*100:.1f}%)\")\n",
    "print(f\"GMM: {gmm.n_components} components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick random image with detections\n",
    "image_paths = list(dataset._index['image_to_detections'].keys())\n",
    "image_path = random.choice(image_paths)\n",
    "detections = dataset.get_detections_for_image(image_path)\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "print(f\"Image: {Path(image_path).name}\")\n",
    "print(f\"Detections: {len(detections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show image with bounding boxes\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.imshow(image)\n",
    "for det in detections:\n",
    "    draw_bbox(ax, det.bbox, color='red', linewidth=2)\n",
    "    # draw_bbox(ax, det.square_crop_bbox, color='blue', linewidth=1)\n",
    "ax.set_title(f\"{len(detections)} detections (red=bbox, blue=square_crop)\")\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show each detection with GMM-colored patches\n",
    "img_w, img_h = image.size\n",
    "n_detections = len(detections)\n",
    "cols = min(4, n_detections)\n",
    "rows = (n_detections + cols - 1) // cols\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n",
    "axes = [axes] if n_detections == 1 else axes.flatten()\n",
    "\n",
    "for idx, det in enumerate(detections):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Get crop\n",
    "    x1, y1, x2, y2 = get_crop_bounds(det.square_crop_bbox, img_w, img_h)\n",
    "    crop = image.crop((x1, y1, x2, y2))\n",
    "    crop_w, crop_h = x2 - x1, y2 - y1\n",
    "    \n",
    "    ax.imshow(crop)\n",
    "    \n",
    "    # Compute GMM responsibilities and colors\n",
    "    resp = compute_patch_responsibilities(det.features, det.patch_mask, pca, gmm)\n",
    "    colors = responsibilities_to_colors(resp)\n",
    "    \n",
    "    # Draw colored patches\n",
    "    coords = patch_coords_in_crop(det.patch_mask, crop_w, crop_h)\n",
    "    draw_patches(ax, coords, colors=colors, alpha=0.6)\n",
    "    \n",
    "    ax.set_title(f\"Det {idx}: {det.patch_mask.sum().item():.0f} patches\")\n",
    "    ax.axis('off')\n",
    "\n",
    "# Hide unused axes\n",
    "for idx in range(n_detections, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fisher vectors\n",
    "import numpy as np\n",
    "from src.data.fv_dataset import FisherVectorDataset\n",
    "\n",
    "fv_dataset = FisherVectorDataset(config.output_root / \"fisher_vectors_reduced\")\n",
    "all_det_ids, all_fvs = fv_dataset.get_all_fisher_vectors()\n",
    "all_fvs_norm = all_fvs / np.linalg.norm(all_fvs, axis=1, keepdims=True)\n",
    "\n",
    "print(f\"Loaded {len(all_det_ids)} Fisher vectors, dim: {all_fvs.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top-5 matches with identity verification (green=correct, red=incorrect)\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def show_detection_with_patches(ax, det, title, alpha=0.4, border_color=None):\n",
    "    img = Image.open(det.image_path).convert('RGB')\n",
    "    img_w, img_h = img.size\n",
    "    x1, y1, x2, y2 = get_crop_bounds(det.square_crop_bbox, img_w, img_h)\n",
    "    crop = img.crop((x1, y1, x2, y2))\n",
    "    crop_w, crop_h = x2 - x1, y2 - y1\n",
    "    \n",
    "    ax.imshow(crop)\n",
    "    \n",
    "    # Draw GMM-colored patches\n",
    "    resp = compute_patch_responsibilities(det.features, det.patch_mask, pca, gmm)\n",
    "    colors = responsibilities_to_colors(resp)\n",
    "    coords = patch_coords_in_crop(det.patch_mask, crop_w, crop_h)\n",
    "    draw_patches(ax, coords, colors=colors, alpha=alpha)\n",
    "    \n",
    "    ax.set_title(title, fontsize=9)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # Add colored border using Rectangle patch\n",
    "    if border_color:\n",
    "        rect = Rectangle((0, 0), crop_w - 1, crop_h - 1, \n",
    "                         linewidth=6, edgecolor=border_color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "for det_idx, query_det in enumerate(detections):\n",
    "    query_identity = identity_map.get(query_det.detection_id)\n",
    "    query_image_uuid = get_image_uuid_from_detection_id(query_det.detection_id)\n",
    "    \n",
    "    # Find top-5 matches (excluding same image)\n",
    "    query_fv = fv_dataset.get_fisher_vector(query_det.detection_id)\n",
    "    query_fv_norm = query_fv / np.linalg.norm(query_fv)\n",
    "    similarities = all_fvs_norm @ query_fv_norm\n",
    "    \n",
    "    # Exclude same-image detections\n",
    "    for i, det_id in enumerate(all_det_ids):\n",
    "        if get_image_uuid_from_detection_id(det_id) == query_image_uuid:\n",
    "            similarities[i] = -np.inf\n",
    "    \n",
    "    top_indices = np.argsort(similarities)[::-1][:5]\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 6, figsize=(18, 3))\n",
    "    \n",
    "    # Query (blue border)\n",
    "    show_detection_with_patches(axes[0], query_det, \n",
    "        f\"Query: {query_identity[:8] if query_identity else 'unknown'}...\", border_color='blue')\n",
    "    \n",
    "    for i, idx in enumerate(top_indices):\n",
    "        match_det = dataset.get_detection(all_det_ids[idx])\n",
    "        match_identity = identity_map.get(all_det_ids[idx])\n",
    "        \n",
    "        # Determine border color: green if same identity, red if different, gray if unknown\n",
    "        if query_identity and match_identity:\n",
    "            border_color = 'green' if query_identity == match_identity else 'red'\n",
    "        else:\n",
    "            border_color = 'gray'\n",
    "        \n",
    "        identity_str = match_identity[:8] if match_identity else 'unknown'\n",
    "        show_detection_with_patches(axes[i+1], match_det, \n",
    "            f\"#{i+1} sim={similarities[idx]:.2f}\\n{identity_str}...\", \n",
    "            border_color=border_color)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute top-k accuracy metrics\n",
    "from src.evaluation import compute_reid_accuracy\n",
    "\n",
    "# Build detection_to_image mapping\n",
    "det_to_image = {det_id: get_image_uuid_from_detection_id(det_id) for det_id in all_det_ids}\n",
    "\n",
    "# Build fisher_vectors dict\n",
    "fisher_vectors = {det_id: all_fvs[i] for i, det_id in enumerate(all_det_ids)}\n",
    "\n",
    "# Compute metrics\n",
    "metrics = compute_reid_accuracy(\n",
    "    fisher_vectors=fisher_vectors,\n",
    "    identity_mapping=identity_map,\n",
    "    exclude_same_image=True,\n",
    "    detection_to_image=det_to_image\n",
    ")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Re-Identification Metrics\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Queries: {metrics.num_queries} (detections with known identity)\")\n",
    "print(f\"Gallery: {metrics.num_gallery}\")\n",
    "print()\n",
    "print(f\"Top-1 Accuracy:  {metrics.top1_accuracy:.2%}\")\n",
    "print(f\"Top-5 Accuracy:  {metrics.top5_accuracy:.2%}\")\n",
    "print(f\"Top-10 Accuracy: {metrics.top10_accuracy:.2%}\")\n",
    "print(f\"Mean Reciprocal Rank: {metrics.mean_reciprocal_rank:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize patch-level matches between query and top-1 match\n",
    "from src.matching import (\n",
    "    extract_valid_patches, compute_patch_similarities, find_matches_ratio_test\n",
    ")\n",
    "from src.visualization import visualize_patch_matches\n",
    "\n",
    "for det_idx, query_det in enumerate(detections):\n",
    "    query_identity = identity_map.get(query_det.detection_id)\n",
    "    query_image_uuid = get_image_uuid_from_detection_id(query_det.detection_id)\n",
    "    \n",
    "    # Find top-1 match (excluding same image)\n",
    "    query_fv = fv_dataset.get_fisher_vector(query_det.detection_id)\n",
    "    query_fv_norm = query_fv / np.linalg.norm(query_fv)\n",
    "    similarities = all_fvs_norm @ query_fv_norm\n",
    "    \n",
    "    for i, det_id in enumerate(all_det_ids):\n",
    "        if get_image_uuid_from_detection_id(det_id) == query_image_uuid:\n",
    "            similarities[i] = -np.inf\n",
    "    k = 10\n",
    "    top1_idx = np.argmax(similarities)\n",
    "    top1_idx = np.argsort(similarities)[::-1][k]\n",
    "    top1_det = dataset.get_detection(all_det_ids[top1_idx])\n",
    "    top1_identity = identity_map.get(all_det_ids[top1_idx])\n",
    "    \n",
    "    # Extract patches (with optional PCA)\n",
    "    patches1, coords1 = extract_valid_patches(query_det.features, query_det.patch_mask, pca=pca)\n",
    "    patches2, coords2 = extract_valid_patches(top1_det.features, top1_det.patch_mask, pca=pca)\n",
    "    \n",
    "    # Compute patch similarities and find matches with ratio test\n",
    "    patch_sims = compute_patch_similarities(patches1, patches2)\n",
    "    matches = find_matches_ratio_test(patch_sims, coords1, coords2, ratio=0.99)\n",
    "    \n",
    "    # Visualize\n",
    "    is_correct = query_identity and top1_identity and query_identity == top1_identity\n",
    "    match_status = \"CORRECT\" if is_correct else \"INCORRECT\" if query_identity and top1_identity else \"UNKNOWN\"\n",
    "    \n",
    "    title = f\"Query: {query_identity[:8] if query_identity else 'unk'}... -> Top-{k}: {top1_identity[:8] if top1_identity else 'unk'}... [{match_status}]\"\n",
    "    fig = visualize_patch_matches(query_det, top1_det, matches, title=title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In notebook - diagnostic\n",
    "ratios = []\n",
    "for i in range(patch_sims.shape[0]):\n",
    "    sorted_sims = np.sort(patch_sims[i])[::-1]\n",
    "    if sorted_sims[0] > 0:\n",
    "        ratios.append(sorted_sims[1] / sorted_sims[0])\n",
    "        \n",
    "print(f\"Ratio distribution: min={min(ratios):.3f}, max={max(ratios):.3f}, mean={np.mean(ratios):.3f}\")\n",
    "# If mean is close to 1.0, features are basically indistinguishable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texture Identity Diagnostic using Fisher Vectors\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from src.config.config import MainConfig\n",
    "from src.data.fv_dataset import FisherVectorDataset\n",
    "from src.evaluation import (\n",
    "    compute_texture_identity_separation,\n",
    "    plot_diagnostic_distributions,\n",
    "    match_detections_to_gt,\n",
    "    get_identity_mapping,\n",
    ")\n",
    "from src.data.coco_loader import COCOLoader\n",
    "from src.data.preprocessed_dataset import PreprocessedDataset\n",
    "\n",
    "# Load configs\n",
    "semantic_config = MainConfig.from_yaml(Path(\"config_zebra_test.yaml\"))  # DINO for pose\n",
    "textural_config = MainConfig.from_yaml(Path(\"config_zebra_disk.yaml\"))  # SIFT for texture\n",
    "\n",
    "# Load semantic Fisher Vectors (DINO)\n",
    "print(\"Loading semantic Fisher Vectors (DINO for pose matching)...\")\n",
    "semantic_fv_dataset = FisherVectorDataset(Path(semantic_config.output_root) / \"fisher_vectors_reduced\")\n",
    "sem_det_ids, sem_fv_matrix = semantic_fv_dataset.get_all_fisher_vectors()\n",
    "semantic_features = {det_id: fv for det_id, fv in zip(sem_det_ids, sem_fv_matrix)}\n",
    "print(f\"Loaded {len(semantic_features)} semantic Fisher Vectors, dim: {sem_fv_matrix.shape[1]}\")\n",
    "\n",
    "# Load textural Fisher Vectors (SIFT)\n",
    "print(\"\\nLoading textural Fisher Vectors (SIFT for texture)...\")\n",
    "textural_fv_dataset = FisherVectorDataset(Path(textural_config.output_root) / \"fisher_vectors_reduced\")\n",
    "tex_det_ids, tex_fv_matrix = textural_fv_dataset.get_all_fisher_vectors()\n",
    "textural_features = {det_id: fv for det_id, fv in zip(tex_det_ids, tex_fv_matrix)}\n",
    "print(f\"Loaded {len(textural_features)} textural Fisher Vectors, dim: {tex_fv_matrix.shape[1]}\")\n",
    "\n",
    "# Get identity mapping (use semantic dataset for GT matching)\n",
    "semantic_dataset = PreprocessedDataset(semantic_config.output_root)\n",
    "coco_loader = COCOLoader(semantic_config.coco_json_path, semantic_config.dataset_root)\n",
    "matched = match_detections_to_gt(semantic_dataset, coco_loader, iou_threshold=0.5, category_ids=[1])\n",
    "identity_map = get_identity_mapping(matched)\n",
    "print(f\"\\nIdentity map: {len(identity_map)} detections with known identity\")\n",
    "\n",
    "# Run diagnostic with Fisher Vectors\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Running texture identity diagnostic with Fisher Vectors\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for skip_k in [0, 5, 10, 20]:\n",
    "    result = compute_texture_identity_separation(\n",
    "        semantic_features=semantic_features,\n",
    "        textural_features=textural_features,\n",
    "        identity_map=identity_map,\n",
    "        n_queries=1000,\n",
    "        k_pose_neighbors=50,\n",
    "        skip_top_k=skip_k,\n",
    "        min_same_identity=1,\n",
    "    )\n",
    "    print(f\"\\nSkip top {skip_k}:\")\n",
    "    print(f\"  ROC-AUC: {result.roc_auc:.4f}\")\n",
    "    print(f\"  Same-ID sim: {result.same_identity_mean:.4f} ± {result.same_identity_std:.4f}\")\n",
    "    print(f\"  Diff-ID sim: {result.diff_identity_mean:.4f} ± {result.diff_identity_std:.4f}\")\n",
    "    print(f\"  Pairs: {result.n_same_identity_pairs} same / {result.n_diff_identity_pairs} diff\")\n",
    "\n",
    "# Plot final result\n",
    "print(\"\\n\")\n",
    "result.print_summary()\n",
    "plot_diagnostic_distributions(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
